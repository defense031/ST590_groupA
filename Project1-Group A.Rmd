---
title: "Project 1-Group A"
author: "Zhen Jiang, Christian Ramsland, Austin Semmel"
date: "Sep.28, 2018"
output:
  html_document:
    toc: yes
    toc_depth: 1
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# What is JSON data, where does it get used, and why is it a good way to store data.
## 
**_JSON_**, short for Javascript Object Notation, is a data format represented as a string that can be used to store ordered lists or name-value pairs. It is intuitive for humans and understood by many computer languages, despite its origins in Javascript.  
It is used most regularly in data interchange, or the transfer of structured information between servers and web applications. Vectors and arrays in JSON format are represented with square brackets and comma delimiters, while objects or name-value pairs use curly brackets and comma delimiters.  
Compared to the comma separated values format, JSON is better at showing hierarchical data and is easy to use with better data reading programs than Microsoft Excel. Compared to XML, JSON is smaller, faster for computers to read, and more intuitive for humans to look at raw. Since many web-based applications are coded using Javascript, at least one end of the channel will natively understand JSON, making it a convenient format for data interchange.  

# Possible packages/functions (pros/cons) that are available for reading JSON data into R.  
**1, The rjson package.**  
   We can use this package for reading in JSON files in R.  
   
**2, The fromJSON function in the rjson package.**  
   See this [link](https://www.tutorialspoint.com/r/r_json_files.htm).  
   **Pros:** We can use this function to easily read in JSON files into R as a list and change it into any data type we want later.  
   **Cons:** when using this function, we can only readin JSON files as list in R, and will have to convert it into atomic vector, data frame, or matrix using as... functions.  
      
**3, The jsonlite package.**  
   See this [link](https://cran.r-project.org/web/packages/jsonlite/vignettes/json-aaquickstart.html).  
   **Pros:** Implements a bidirectional mapping between JSON data and the most important R data types. Thereby we can convert between R objects and JSON without loss of type or information, and without the need for any manual data munging.  
   
**4, The fromJSON function in the jsonlite package.**  
   **Pros:** converts JSON arrays automatically into different types of R objects.  
   This function has 3 arguments which control the converting process: simplifyVector, simplifyDataFrame and simplifyMatrix for converting JSON data into atomic vector, data frame, and matrix in R separately.  
   **Cons:** need to provide more arguments when using the function to determine what kind of data type we want in R.  

We would like to choose `fromJSON()` function from the jsonlite package, because we can read in the json data and convert it to the data type we want at the same time.  

# Example of reading in a JSON file & summarize the data set.
## Description of the dataset.
This dataset comes from a github library of various JSON formatted files.  
It is based on data for the game Pokemon Go, and contains information about the original 151 Pokemon.  

## Read in the json file  

**Load libraries.**  
```{r message=FALSE}
library(jsonlite)
```

**Read in the json file**
```{r}
poke <- fromJSON("https://raw.githubusercontent.com/Biuni/PokemonGO-Pokedex/master/pokedex.json",simplifyDataFrame = TRUE)
Pokedf <- poke$pokemon
```

## Create a new variable:  
**New variable: avg_spawn_chance. This represents the average spawn chance for each candy type.**  

**Load libraries.**  
```{r message=FALSE}
library(dplyr)
library(ggplot2)
library(tidyr)
```

```{r}
PokeCandy<-group_by(Pokedf,candy)%>%mutate(AvgSpawnChance=mean(spawn_chance))
```

**Show the name & candy column as well as the new variable**  
```{r}
select(PokeCandy, name, candy, AvgSpawnChance)
```

## Create a function & contingency table
```{r}
getTypes <- function(df){
  type1 <- (unlist(df[5])[1])
  type2 <- (unlist(df[5])[2])
  Typesdf <- c(type1,type2)
}

newCols <-apply(Pokedf,1,getTypes)
newCols[is.na(newCols)] <- 0
newCols <- t(newCols)
colnames(newCols) <- c("type1","type2")
Pokedf <- cbind(Pokedf,newCols)
Pokedf <- data.frame(lapply(Pokedf,as.character),stringsAsFactors = FALSE)
table(Pokedf$type1,Pokedf$type2)
```

## Numeric Summaries:  

**Numeric summary of Average Spawns by primary pokemon type.**
```{r}
avgSpawnSummary = c()
typevector <- c("Normal","Grass","Flying","Poison","Electric","Ground","Fire","Water","Dragon","Ice","Ghost","Psychic","Rock","Bug","Fighting")
for (i in typevector){
  avgSpawnSummary <- c(avgSpawnSummary,(summary(as.numeric((filter(Pokedf,(type1==i)|(type2==i)))$avg_spawns))))}
avgSpawnSummary <- cbind(typevector,(matrix(avgSpawnSummary, ncol = 6, byrow = TRUE)))
colnames(avgSpawnSummary) <- c("Type","Min.","1st Qu.","Median","Mean","3rd Qu.","Max.")
```

**Numeric summary of mean of spawn chance by pokemon type.**
```{r}
#Create list of unique types
types<-unique(Pokedf$type1)

#Initialize dataframe to capture mean values
sumData<-data.frame(matrix(ncol=14,nrow=1))
colnames(sumData)<-as.character(types)

#Find mean spawn chance for each type
for (i in types){
  newDF<-filter(Pokedf,(type1==i)|(type2==i))
  sumData[i]<-mean(as.numeric(newDF$spawn_chance))
  
}
as_tibble(sumData)
```

## Create a bar plot:  
**Bar plot showing means of average spawns for each pokemon type.**
```{r}
dfAvgSpawnSummary<-as.data.frame(avgSpawnSummary)
p<-ggplot(dfAvgSpawnSummary, aes(x=Type, y=Mean))
p+geom_bar(stat="identity", fill="blue")+
labs(x="Type", title="Barplot of mean average spawns by pokemon type")+
ylab("Mean of average spawns")+
theme(axis.text.x=element_text(angle=90))
```


## Create a box plot:  
**Box plot showing spawn chance for each candy type taken by pokemon.**  
```{r}
b<-ggplot(PokeCandy, aes(x=candy, y=spawn_chance))
b+geom_boxplot(fill="gold1")+
geom_jitter()+
labs(x="Candy Type", title="Boxplot of spawn chance by candy type")+
ylab("Spawn Chance")+
theme(axis.text.x=element_text(angle=90,hjust=1, vjust=0.5))
```

## Create a scatter plot:  
**Scatter plot showing height vs weight, colored by the primary pokemon type.**
```{r}
ggplot(Pokedf, aes(x = (as.numeric(gsub("kg","",Pokedf[,7]))), y = (as.numeric(gsub("m","",Pokedf[,6]))),color = type1)) + geom_point()+labs(x="weight (kg)",y="height (m)",color = "Main Type", title = "Pokemon Height and Weight with Primary Typing")

```

#Second example
####This dataset is contains information on the effectiveness and placement of Russian ads
#### Reading JSON and wrangling:

####Load additional libraries
```{r}
library(dplyr)
library(tidyr)
```
####Read in the json file
```{r Read}
## Group D ("Amanda, Adil & Brian") - Project 1b:
digital <- fromJSON("https://www4.stat.ncsu.edu/~post/558/datasets/russian_ads.json", flatten = TRUE)


# As you look closer you would notice the first 4 columns (variables) are ‘list’ data type and they are ‘nested’. This is because each ‘categories’ columns include an JS array data object in the original JSON data.
# For Example (Look at the middle bracket[])

# "language_categories": [
#       "Not Specified"
#     ],
#     "placement_categories": [
#       "Facebook"
#     ],
#     "interests_categories": [],
#     "location_categories": [
#       "Midwest",
#       "Atlantic"
#     ]

# So even after the flattening the data, this type of variables are registered as ‘list’ data type and it has a list of the values (or atomic vector) for each row of the data frame. You can easily show the values inside of this type of variable by using as.character() function like below.

# Convert to tibble
digital_tb <- as_data_frame(digital)

# As an example
digital_tb %>% dplyr::mutate(location_categories = as.character(location_categories)) %>% dplyr::select(location_categories)

# To break out ‘categories’ list columns and create one row for each value, we can use ‘unnest()’ function from tidyr package.

# unnest 4 columns using stepwise approach. .drop = FALSE keeps other list columns that have not been unnested yet in the process.
digital_tb_unnest_noempty <- digital_tb %>% unnest(language_categories, .drop = FALSE) %>% unnest(placement_categories, .drop = FALSE) %>% unnest(interests_categories, .drop = FALSE) %>% unnest(location_categories, .drop = FALSE)

# Unnested columns are at the end of the dataset

dim(digital_tb_unnest_noempty)

# Checking dimension, there are 1803 observations, which is less than 3517 from the original parsed dataset. This is due to 3rd and 4th columns containing character(0), i.e. character array with 0 length, these 2 columns can't be unnested by default unnest(). Otherwise, it will drop all the rows with 1 or more empty data points in those list columns.

# We can use the _regex version of interests_categories and locations_categories, then use detect string functions for aggregation

# Or try to unnest the rows without empty points and join back to the master dataset. This seems to be a more holistic solution.

# Check if ad_id is unique
sum(table(digital_tb$ad_id)!=1)

# returns 0. ad_id can theoretically serve as a join key

# Wait!
sum(is.na(digital_tb$ad_id))
# unfortunately, there are 151 NAs in ad_id. Let's create our own row_id.

digital_tb <- digital_tb %>% mutate(own_id = rownames(digital_tb)) %>% select(own_id, everything())

# Grab language_categories list column and own_id, unnest it, assign it to table_inter1.
# unnest() by default will remove empty points character(0), but explicitly filtering first would result in faster execution. It probably does not matter in this case.
# Also, be aware that lengths() is different from length(). lengths() checks the length of each element of the list column
table_inter1 <- digital_tb %>% select(language_categories, own_id) %>% filter(lengths(language_categories) > 0) %>% unnest()

# If the unnest works properly, there should be a row for each value in the vectors.
# Check if the number of rows of table_inter1 is equal to the sum of array elements.
sum(lengths(digital_tb$language_categories)) == dim(table_inter1)[1]

# TRUE. We are good. Apply the same method to the other 3 columns.
table_inter2 <- digital_tb %>% select(placement_categories, own_id) %>% filter(lengths(placement_categories) > 0) %>% unnest()

table_inter3 <- digital_tb %>% select(interests_categories, own_id) %>% filter(lengths(interests_categories) > 0) %>% unnest()

table_inter4 <- digital_tb %>% select(location_categories, own_id) %>% filter(lengths(location_categories) > 0) %>% unnest()

# Left table is master. Right table is the table of unnested column and own_id.
digital_fully_unnest <- digital_tb %>% left_join(table_inter1, by = "own_id") %>% left_join(table_inter2, by = "own_id") %>% left_join(table_inter3, by = "own_id") %>% left_join(table_inter4, by = "own_id")

# It works. The resulting tibble is fully unnested. Rows are duplicated properly for each unique combination of category values. 

# Remove the list columns (.x suffix) from the original left table and put freshly joined columns (.y suffix) at front.
df <- digital_fully_unnest %>% select(-ends_with('.x')) %>% select(ends_with('.y'), everything())
colnames(df) = gsub(".y", "", colnames(df))

# Use df for further analysis

```

## Exploratory analysis and data visualization
```{r}
# Warning!!!!
# since data is now fully unnested, the data (e.g. ad spend) of an ad would repeat itself for each unique combination of category values. We need to becareful about double counting.

# For example
df %>% filter(ad_id == 1147) %>% select(ends_with("categories"), ad_spend_usd)

# To avoid the problem of double counting in aggregation analysis that involves a subset of categories, pull the distinct rows (in this case ads) of each group.

# For example, summarize using only interests. Get the unique data for the interest group first.
# .keep_all = TRUE retains all other columns associated to the unique ad.
int_summary_interests <- df %>% group_by(interests_categories) %>% distinct(own_id, .keep_all = TRUE)

# Check to see if this mindset works
int_summary_interests %>% filter(ad_id == 1147)

# It shows only 2 rows associated to ad_id 1147 becasue 1147 has only 2 interest categories, instead of 8 rows in the comprehensive df. This is the result we want. Now we can do a summarization such as total amount spent in each interest category
summary_interests <- int_summary_interests %>% summarize(total_spend_usd = sum(ad_spend_usd, na.rm = TRUE))
summary_interests
# You can summarize other columns in sum(), mean(), count(), etc by changing the above line.

# Another example, summarize using 2 categories, location and interests. 
# Doing it in 1 line
summary_locations_interests <- df %>% group_by(location_categories, interests_categories) %>% distinct(own_id, .keep_all = TRUE) %>% summarize(average_efficien_impressions = mean(efficien_impressions, na.rm = TRUE))
summary_locations_interests

# From now on, you can analyze anything you want with flexibility using the unnested data structure.

```

```{r}
# Mean conversion rate by interest category
conver_rate_by_cat<- df %>% group_by(interests_categories) %>% distinct(own_id, .keep_all = TRUE) %>% summarise(mean_conversion_rate = mean(conversion_rate, na.rm = TRUE))

g <- ggplot(conver_rate_by_cat, aes(x=interests_categories, y=mean_conversion_rate))
g + geom_point() + coord_flip() + labs(x= "Interest Categories", y = "Mean Conversion Rate", title = "Mean Conversion Rate by Interest")

# Mean conversion by region
mean_conversions_region<- df %>% group_by(location_categories) %>% distinct(own_id, .keep_all = TRUE) %>% summarise(mean_conversion_rate = mean(conversion_rate, na.rm = TRUE))
g4<-ggplot(mean_conversions_region, aes(x=location_categories, y=mean_conversion_rate))
g4 + geom_point()+labs(x= "Region", y = "Mean Conversion Rate", title = "Mean Conversion Rate by Region")

# Mean conversion by placement media
mean_conversions_placement<- df %>% group_by(placement_categories) %>% distinct(own_id, .keep_all = TRUE) %>% summarise(mean_conversion_rate = mean(conversion_rate, na.rm = TRUE)) %>% arrange(mean_conversion_rate) 

#this may be more appropriate for a table.  proably should limit it to top ten.  can also do this for impressions and maybe by add #
knitr::kable(mean_conversions_placement)

```

```{r, warning= FALSE}
# Count of ads in each interest category broken by location category
distinct_summary_locations_interests <- df %>% group_by(location_categories, interests_categories) %>% distinct(own_id, .keep_all = TRUE)

g2<-ggplot(distinct_summary_locations_interests,aes(x=interests_categories))
g2+geom_bar(aes(fill = as.factor(location_categories)))+coord_flip() + labs(x = "Interest Categories", y = "Count", title = "Interest Categories by Region") + scale_fill_discrete(name = "Region")

#impressions by region
locations_interests_dropna<- distinct_summary_locations_interests %>% drop_na(location_categories)
g3<-ggplot(locations_interests_dropna, aes(x=location_categories, y=efficien_impressions))
g3+geom_boxplot() + labs(x = "Location Categories", y = "Impressions", title = "Impressions by Region")

```
```{r Business, warning = FALSE}
# Trying to figure out if the number of clicks contributes to the effectiveness of the ad campaigns.  In other words, does the number of ad clicks correlate to the conversion rate
# First Creating a subset of df calling it dfEffective
dfEffective <- df %>% select(ad_clicks, conversion_rate)
#Plotting the two numeric variables and adding a regression line 
g<-ggplot(dfEffective, aes(x=ad_clicks, y=conversion_rate))
g + geom_point() + geom_smooth(method='lm',formula=y~x) + labs(x="Ad Clicks", y = "Conversion Rate", title = "Conversion Rate by Number of Clicks")
# Visual suggests there is an association between the two variables, not necessarily causation.  Further analysis and optimization could show how much variance of the conversion rate can be explained by the number of ad clicks.
```


